{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30019d31-6989-4ead-919f-ba8f2dcc3a9a",
   "metadata": {},
   "source": [
    "# Data Visualisation of COVID-19 in the UK using Sqlite, Pandas and Seaborn Libraries \n",
    "## Part 2a: Extract the Data\n",
    "\n",
    "This is the second notebook in a 3-part series which explores the UK Gov's COVID-19 dashboard data which is publically available for download via a REST API from gov.uk.  We make use of a Sqlite3 database to query this data using SQL and import aggregations into Pandas data frames.  We then use the Seaborn library to visualise the results.\n",
    "\n",
    "In this notebook (Part 2) - we first extract the data using a REST API based on the GOV.UK COVID-19 SDK and then insert the data into empty tables created in Part 1.  Part 3 will then query and visualise the data.\n",
    "\n",
    "The data used in this notebook is publically available and more information can be found here:\n",
    "https://coronavirus.data.gov.uk/details/about-data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3578f5-9bfa-41f9-877f-2a2140766ad3",
   "metadata": {},
   "source": [
    "### Configuration and Setup\n",
    "\n",
    "The main library we will use to perform the REST API calls to get the GOV.UK COVID-19 data is the SDK published by GOV.UK.  This will handle the various calls, but also pagination as well and populate the data into a Pandas data-frame.\n",
    "\n",
    "Install the library using `pip install uk-covid19`\n",
    "\n",
    "For more information on the SDK, see: https://pypi.org/project/uk-covid19/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b4dfe7f4-d20f-4dd2-8bbf-2b57b91c3944",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uk_covid19 import Cov19API  # Use the UK COVID-19 GOV.UK SDK\n",
    "import pandas as pd              # Pandas library for Dataframes\n",
    "import time                      # Need the Sleep function for API calls\n",
    "import csv                       # For CSV generation\n",
    "from datetime import datetime    # Datetime functionality\n",
    "import re                        # Regular Expression for the snake_case function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ba4d6b-6eaa-4693-bcff-cc7060bb0288",
   "metadata": {},
   "source": [
    "### Camel Case to Snake Case Function for Column Names\n",
    "\n",
    "We're going to use a naming standard for the SQL tables based on Snake Case (lowercase and underscore only).  One reason we might do this is because it's more compliant with various database systems such as HiveQL, Amazon Athena, etc.\n",
    "\n",
    "Unfortunately the column naming used by the GOV.UK API is based on Camel Case (mixed case with no spaces).  So the following function will be used to convert the source column names into snake case which will make importing the data easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f868d0a8-3b75-489e-b056-6777806fd896",
   "metadata": {},
   "outputs": [],
   "source": [
    "def camel_to_snake(name: str) -> str:\n",
    "    \"\"\"\n",
    "    Description: Convert any string to snake case (lower case and _ for spacing)\n",
    "    Args:        name: input string to convert\n",
    "    Returns:     input string converted to snake case \n",
    "    \"\"\"   \n",
    "    name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    name = re.sub('[,.]', '_', name)   \n",
    "    name = re.sub('[+*&%=()?<>!@#$/\\\\\\\\]', '', name)  \n",
    "\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name).lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\"__\",\"_\").replace(\"__\",\"_\").replace(\"deaths28\",\"deaths_28\")       "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7614956-8128-4efe-89c5-917056f565c1",
   "metadata": {},
   "source": [
    "### Handling Metrics and Area Types in the REST API\n",
    "\n",
    "If you read the documentation provided at https://coronavirus.data.gov.uk/details/download and https://coronavirus.data.gov.uk/details/developers-guide - you'll see that only a maximum of 5 metrics can be requested via the API in addition to the standard (primary key) metrics: Area Type, Name, Code and Date.\n",
    "\n",
    "This means we have to make multiple API requests for groups of metrics since we can't request all in one go.  Additionally the metrics available differ depending on the Area Type you are requesting.\n",
    "\n",
    "The following provides a configuration in the form of a dictionary object which define the metrics available for each area type.  In addition the metrics are broken down into groups of no more than 5 metrics and this will allow us to iterate in a more generic function.  The default metrics are also provided in a separate list object.\n",
    "\n",
    "This notebook is going to retrieve data for all area types, and for most of the metrics available.  However, this is forever changing so as more metrics become available we simply add them to one or more separate metric groups for the appropriate area type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5cfd0f6c-033b-48d4-b3d0-ac50491dd2fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "default_metrics = [\"areaType\",\n",
    "                   \"areaName\",\n",
    "                   \"areaCode\",\n",
    "                   \"date\"]\n",
    "\n",
    "metrics_by_area_type = {\n",
    "    \"overview\" : [\n",
    "        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
    "        [\"cumCasesBySpecimenDateRate\",\"newPillarOneTestsByPublishDate\",\"cumPillarOneTestsByPublishDate\"],\n",
    "        [\"newPillarTwoTestsByPublishDate\",\"cumPillarTwoTestsByPublishDate\",\"newPillarThreeTestsByPublishDate\",\"cumPillarThreeTestsByPublishDate\",\"newPillarFourTestsByPublishDate\"],\n",
    "        [\"cumPillarFourTestsByPublishDate\",\"newAdmissions\",\"cumAdmissions\",\"cumTestsByPublishDate\"],\n",
    "        [\"newTestsByPublishDate\",\"covidOccupiedMVBeds\",\"hospitalCases\",\"plannedCapacityByPublishDate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
    "        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
    "    ],\n",
    "    \"nation\" : [\n",
    "        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
    "        [\"cumCasesBySpecimenDateRate\",\"maleCases\",\"femaleCases\",\"newPillarOneTestsByPublishDate\",\"cumPillarOneTestsByPublishDate\"],\n",
    "        [\"newPillarTwoTestsByPublishDate\",\"cumPillarTwoTestsByPublishDate\",\"newPillarThreeTestsByPublishDate\",\"cumPillarThreeTestsByPublishDate\"],\n",
    "        [\"newAdmissions\",\"cumAdmissions\",\"cumAdmissionsByAge\",\"cumTestsByPublishDate\"],\n",
    "        [\"newTestsByPublishDate\",\"covidOccupiedMVBeds\",\"hospitalCases\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
    "        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
    "    ],\n",
    "    \"region\" : [\n",
    "        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
    "        [\"cumCasesBySpecimenDateRate\",\"maleCases\",\"femaleCases\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
    "        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
    "    ],  \n",
    "    \"nhsRegion\" : [\n",
    "        [\"newAdmissions\",\"cumAdmissions\",\"cumAdmissionsByAge\",\"covidOccupiedMVBeds\",\"hospitalCases\"]\n",
    "    ],  \n",
    "    \"utla\" : [\n",
    "        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
    "        [\"cumCasesBySpecimenDateRate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
    "        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
    "    ],\n",
    "    \"ltla\" : [\n",
    "        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
    "        [\"cumCasesBySpecimenDateRate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
    "        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
    "    ]    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ca0c466-f8c3-4f5e-b54a-dc9042a3b530",
   "metadata": {},
   "source": [
    "### Extract the Data as CSV by iterating through the metric groups\n",
    "\n",
    "Now we simply iterate through each area type which provides a list of metric groups.  Then we iterate through each metric group (no more than 5 metrics) and combined with the mandatory default metrics we use the SDK to make a REST API call and grab the data into a Pandas dataframe.\n",
    "\n",
    "Unfortunately there is rate limiting implemented for this API as documentated in the fair usage policy: https://coronavirus.data.gov.uk/details/download.  As such we use `time.sleep(1)` to pause for one second between each API call.  So far this has provded sufficient to abide by the fair usage policy.\n",
    "\n",
    "As we iterate through each metric group, we combine all the data together using an outer join merge based on the default metric columns.\n",
    "\n",
    "We then convert the column names to snake case, convert the `date` column to datetime, and finally write the data out to a CSV (so one CSV per area type is generated).\n",
    "\n",
    "**So why CSV and not load straight to the database?** \n",
    "\n",
    "It's common in most ETL/ELT processing systems to initially stage the data before loading into a database - typically to a data-lake of some sort.  Here we simply use the file system and stage the data as CSV.  This means if we have any errors loading the data into the database tables, we have the data in a ready to use format to investigate the issues further.  \n",
    "\n",
    "**NOTE:** The following code will take several minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d0343f-2914-4bc5-a258-0a5e7d9bee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through each area type (this provides a list of metric groups)\n",
    "# key = areaType, value = list of metric groups (or a list of lists of metrics)\n",
    "for key, value in metrics_by_area_type.items():\n",
    "    \n",
    "    filters = [f\"areaType={key}\"]\n",
    "    \n",
    "    df_merged = pd.DataFrame()\n",
    "    \n",
    "    # iterate through each metric group (at most 5 metrics)\n",
    "    metric_group_index = 1\n",
    "    for metric_group in value:\n",
    "        # we create a dictionary containing the metrics which must also include the default metrics\n",
    "        structure = {}\n",
    "        structure_d = {k: k for k in default_metrics}\n",
    "        structure_m = {k: k for k in metric_group}\n",
    "        structure.update(structure_d)\n",
    "        structure.update(structure_m)\n",
    "\n",
    "        # call the API using the SDK - requires just the Area Type and Metrics requested \n",
    "        api = Cov19API(\n",
    "            filters=filters,\n",
    "            structure=structure\n",
    "        ) \n",
    "        \n",
    "        # grab data as a Pandas dataframe and merge with any previously requested metrics\n",
    "        df = api.get_dataframe() \n",
    "        print(f\"{datetime.now().strftime('%H:%M:%S')} : Area Type: {key} - Metric Group {str(metric_group_index)} - Num records: {len(df)}\")\n",
    "        if df_merged.empty:\n",
    "            df_merged = df\n",
    "        else:\n",
    "            df_merged = df_merged.merge(df, on=default_metrics, how=\"outer\")\n",
    "            \n",
    "        # pause for a second to avoid breaching the fair usage policy\n",
    "        time.sleep(1.0)\n",
    "        metric_group_index += 1\n",
    "     \n",
    "    # all the metrics have now been retrieved and merged so convert columns to snake_case and write out the area type's CSV data\n",
    "    df_merged.rename(columns=camel_to_snake, inplace=True)\n",
    "    df_merged[\"date\"]= pd.to_datetime(df_merged[\"date\"])\n",
    "    df_merged.to_csv(f\"{key}.csv\", index=False, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682a5f6-5461-4918-9802-739f0fc2b940",
   "metadata": {},
   "source": [
    "### Supplementary Downloads\n",
    "\n",
    "We need to download population data for the various geographies (such as UTLA) which are available from GOV.UK.  This will help provide standardised case and death rates between different areas.  \n",
    "\n",
    "As was the case for the COVID-19 data, we snake case the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "066b7b26-f78a-47f9-b81c-5fd55345eb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_populations = pd.read_csv(f\"https://coronavirus.data.gov.uk/downloads/supplements/ONS-population_2021-08-05.csv\")\n",
    "df_populations.rename(columns=camel_to_snake, inplace=True)\n",
    "df_populations.to_csv(f\"populations.csv\", index=False, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d53125-31a1-4d82-ac5d-d4801ace6752",
   "metadata": {},
   "source": [
    "## Part 2b: Load the Data\n",
    "\n",
    "Now we have the data staged a set of CSV files.  All that is left to load these in to the sqlite database that was created by the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f37d767c-596b-44a2-977c-4434db5ce686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "sqlite_db_path = \"c19.db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109845eb-f3e8-4ccd-adcb-150999639c2d",
   "metadata": {},
   "source": [
    "### Optional: Uncompress Sqlite database file \n",
    "\n",
    "If you compressed the database in the previous notebook, then you'll need to first decompress the file before opening it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "239698a8-d55e-4c45-a227-784307effd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gzip\n",
    "import shutil\n",
    "\n",
    "with gzip.open(sqlite_db_path + '.gz', 'rb') as f_in:\n",
    "    with open(sqlite_db_path, 'wb') as f_out:\n",
    "        shutil.copyfileobj(f_in, f_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "604d53be-9686-4500-99dc-8dbe499f4f93",
   "metadata": {},
   "source": [
    "### Open the database connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1e84c792-355d-44ee-b3e0-2fd3af485985",
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(sqlite_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c35c1f0-a7aa-4b63-869c-3d5daaafe36d",
   "metadata": {},
   "source": [
    "### Load the CSV files into the database\n",
    "\n",
    "Now we simply load the data into the database using intermediary Pandas dataframes.  Note this assumes the tables are currently empty - the CSV file is appended to the tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "70a7d0c6-af3b-4db5-b0d6-d9e48134f813",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(f\"nation.csv\").to_sql(\"c19dashboard_uk__national_daily_metrics\", conn, if_exists=\"append\", index=False)\n",
    "pd.read_csv(f\"overview.csv\").to_sql(\"c19dashboard_uk__summary_daily_metrics\", conn, if_exists=\"append\", index=False)\n",
    "pd.read_csv(f\"nhsRegion.csv\").to_sql(\"c19dashboard_uk__nhsregion_daily_metrics\", conn, if_exists=\"append\", index=False)\n",
    "pd.read_csv(f\"region.csv\").to_sql(\"c19dashboard_uk__region_daily_metrics\", conn, if_exists=\"append\", index=False)\n",
    "pd.read_csv(f\"utla.csv\").to_sql(\"c19dashboard_uk__utla_daily_metrics\", conn, if_exists=\"append\", index=False)\n",
    "pd.read_csv(f\"ltla.csv\").to_sql(\"c19dashboard_uk__ltla_daily_metrics\", conn, if_exists=\"append\", index=False)\n",
    "pd.read_csv(f\"populations.csv\").to_sql(\"reference_geography__age_gender_populations\", conn, if_exists=\"append\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00ffa9a-d97c-4a5f-9175-ba6cd5d34167",
   "metadata": {},
   "source": [
    "### Cleanup\n",
    "\n",
    "Ensure all changes are committed and then close the Sqlite connection.  Also force garbage collection - at this point, there should be no locks on the database file so it could be zipped up and deleted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cab9bb07-b1f7-4d52-ba82-73ac50a0c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "# Commit and close Sqlite connection\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "# Force garbage collection\n",
    "_ = gc.collect(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

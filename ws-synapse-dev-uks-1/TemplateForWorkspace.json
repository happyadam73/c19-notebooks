{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "ws-synapse-dev-uks-1"
		},
		"ws-synapse-dev-uks-1-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ws-synapse-dev-uks-1-WorkspaceDefaultSqlServer'"
		},
		"ws-synapse-dev-uks-1-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://sasynapsedevuks1.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/ws-synapse-dev-uks-1-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('ws-synapse-dev-uks-1-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws-synapse-dev-uks-1-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ws-synapse-dev-uks-1-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/COVID 19 Data Viz using Azure Synapse - Part 1 - Extract and Load the Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Data Visualisation of COVID-19 in the UK using Azure Synapse Spark Notebooks \n",
							"## Part 1: Extract and Load the Data\n",
							"\n",
							"This is the first notebook in a 2-part series which explores the UK Gov's COVID-19 dashboard data which is publically available for download via a REST API from gov.uk.  \n",
							"Using PySpark and the GOV.UK COVID-19 SDK we extract the data and load to a set of Spark Sql tables.  \n",
							"\n",
							"In the second notebook of the series, we then use the Seaborn library to visualise the results.\n",
							"\n",
							"The data used in this notebook is publically available and more information can be found here:\n",
							"https://coronavirus.data.gov.uk/details/about-data"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Configuration and Setup\n",
							"\n",
							"The main library we will use to perform the REST API calls to get the GOV.UK COVID-19 data is the SDK published by GOV.UK.  This will handle the various calls, but also pagination as well and populate the data into a Pandas data-frame.\n",
							"\n",
							"Install the library using `pip install uk-covid19` - in Synapse you may want to either add this as a workspace package, or install as a session configuration via an environment.yml file.  Currently this notebook has been tested on version 1.2.2.\n",
							"\n",
							"For more information on the SDK, see: https://pypi.org/project/uk-covid19/"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from uk_covid19 import Cov19API  # Use the UK COVID-19 GOV.UK SDK\n",
							"import pandas as pd              # Pandas library for Dataframes\n",
							"import time                      # Need the Sleep function for API calls\n",
							"import csv                       # For CSV generation\n",
							"from datetime import datetime    # Datetime functionality\n",
							"import re                        # Regular Expression for the snake_case function"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Camel Case to Snake Case Function for Column Names\n",
							"\n",
							"We're going to use a naming standard for the SQL tables based on Snake Case (lowercase and underscore only).  One reason we might do this is because it's more compliant with various database systems such as HiveQL, Amazon Athena, etc.\n",
							"\n",
							"Unfortunately the column naming used by the GOV.UK API is based on Camel Case (mixed case with no spaces).  So the following function will be used to convert the source column names into snake case which will make importing the data easier."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"def camel_to_snake(name: str) -> str:\n",
							"    \"\"\"\n",
							"    Description: Convert any string to snake case (lower case and _ for spacing)\n",
							"    Args:        name: input string to convert\n",
							"    Returns:     input string converted to snake case \n",
							"    \"\"\"   \n",
							"    name = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n",
							"    name = re.sub(\"[,.]\", \"_\", name)   \n",
							"    name = re.sub(\"[+*&%=()?<>!@#$/\\\\\\\\]\", \"\", name)  \n",
							"\n",
							"    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name).lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\"__\",\"_\").replace(\"__\",\"_\").replace(\"deaths28\",\"deaths_28\")   "
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Handling Metrics and Area Types in the REST API\n",
							"\n",
							"If you read the documentation provided at https://coronavirus.data.gov.uk/details/download and https://coronavirus.data.gov.uk/details/developers-guide - you'll see that only a maximum of 5 metrics can be requested via the API in addition to the standard (primary key) metrics: Area Type, Name, Code and Date.\n",
							"\n",
							"This means we have to make multiple API requests for groups of metrics since we can't request all in one go.  Additionally the metrics available differ depending on the Area Type you are requesting.\n",
							"\n",
							"The following provides a configuration in the form of a dictionary object which define the metrics available for each area type.  In addition the metrics are broken down into groups of no more than 5 metrics and this will allow us to iterate in a more generic function.  The default metrics are also provided in a separate list object.\n",
							"\n",
							"This notebook is going to retrieve data for all area types, and for most of the metrics available.  However, this is forever changing so as more metrics become available we simply add them to one or more separate metric groups for the appropriate area type."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"default_metrics = [\"areaType\",\n",
							"                   \"areaName\",\n",
							"                   \"areaCode\",\n",
							"                   \"date\"]\n",
							"\n",
							"metrics_by_area_type = {\n",
							"    \"overview\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"newPillarOneTestsByPublishDate\",\"cumPillarOneTestsByPublishDate\"],\n",
							"        [\"newPillarTwoTestsByPublishDate\",\"cumPillarTwoTestsByPublishDate\",\"newPillarThreeTestsByPublishDate\",\"cumPillarThreeTestsByPublishDate\",\"newPillarFourTestsByPublishDate\"],\n",
							"        [\"cumPillarFourTestsByPublishDate\",\"newAdmissions\",\"cumAdmissions\",\"cumTestsByPublishDate\"],\n",
							"        [\"newTestsByPublishDate\",\"covidOccupiedMVBeds\",\"hospitalCases\",\"plannedCapacityByPublishDate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
							"    ],\n",
							"    \"nation\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"maleCases\",\"femaleCases\",\"newPillarOneTestsByPublishDate\",\"cumPillarOneTestsByPublishDate\"],\n",
							"        [\"newPillarTwoTestsByPublishDate\",\"cumPillarTwoTestsByPublishDate\",\"newPillarThreeTestsByPublishDate\",\"cumPillarThreeTestsByPublishDate\"],\n",
							"        [\"newAdmissions\",\"cumAdmissions\",\"cumTestsByPublishDate\"],\n",
							"        [\"newTestsByPublishDate\",\"covidOccupiedMVBeds\",\"hospitalCases\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"],\n",
							"        [\"newPeopleVaccinatedFirstDoseByPublishDate\",\"newPeopleVaccinatedSecondDoseByPublishDate\",\"cumPeopleVaccinatedFirstDoseByPublishDate\",\"cumPeopleVaccinatedSecondDoseByPublishDate\"]\n",
							"    ],\n",
							"    \"region\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
							"    ],  \n",
							"    \"nhsRegion\" : [\n",
							"        [\"newAdmissions\",\"cumAdmissions\",\"covidOccupiedMVBeds\",\"hospitalCases\"]\n",
							"    ],  \n",
							"    \"utla\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
							"    ],\n",
							"    \"ltla\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
							"    ]    \n",
							"}"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Extract and Load the Data via REST API by iterating through the metric groups\n",
							"\n",
							"Now we simply iterate through each area type which provides a list of metric groups.  Then we iterate through each metric group (no more than 5 metrics) and combined with the mandatory default metrics we use the SDK to make a REST API call and grab the data into a Pandas dataframe.\n",
							"\n",
							"Unfortunately there is rate limiting implemented for this API as documentated in the fair usage policy: https://coronavirus.data.gov.uk/details/download.  As such we use `time.sleep(1)` to pause for one second between each API call.  So far this has provded sufficient to abide by the fair usage policy.\n",
							"\n",
							"As we iterate through each metric group, we combine all the data together using an outer join merge based on the default metric columns.  In order to avoid Spark data type errors, we convert List based columns to strings.\n",
							"\n",
							"We then convert the column names to snake case, convert the `date` column to datetime, and finally write the data out to the C19 Spark database by converting the Pandas dataframe to a Spark dataframe.\n",
							"\n",
							"**NOTE:** The following code will take several minutes to complete."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# iterate through each area type (this provides a list of metric groups)\n",
							"# key = areaType, value = list of metric groups (or a list of lists of metrics)\n",
							"for key, value in metrics_by_area_type.items():\n",
							"    \n",
							"    filters = [f\"areaType={key}\"]\n",
							"    \n",
							"    df_merged = pd.DataFrame()\n",
							"    \n",
							"    # iterate through each metric group (at most 5 metrics)\n",
							"    metric_group_index = 1\n",
							"    for metric_group in value:\n",
							"        # we create a dictionary containing the metrics which must also include the default metrics\n",
							"        structure = {}\n",
							"        structure_d = {k: k for k in default_metrics}\n",
							"        structure_m = {k: k for k in metric_group}\n",
							"        structure.update(structure_d)\n",
							"        structure.update(structure_m)\n",
							"\n",
							"        # call the API using the SDK - requires just the Area Type and Metrics requested \n",
							"        api = Cov19API(\n",
							"            filters=filters,\n",
							"            structure=structure\n",
							"        ) \n",
							"        \n",
							"        # grab data as a Pandas dataframe and merge with any previously requested metrics\n",
							"        df = api.get_dataframe() \n",
							"\n",
							"        # convert all list type columns to string\n",
							"        list_col_map = df.applymap(lambda x: isinstance(x, list)).all()\n",
							"        for col in list_col_map.index[list_col_map].tolist():\n",
							"            df[col] = df[col].astype(str)\n",
							"\n",
							"        print(f\"{datetime.now().strftime('%H:%M:%S')} : Area Type: {key} - Metric Group {str(metric_group_index)} - Num records: {len(df)}\", flush=True)\n",
							"        if df_merged.empty:\n",
							"            df_merged = df\n",
							"        else:\n",
							"            df_merged = df_merged.merge(df, on=default_metrics, how=\"outer\")\n",
							"            \n",
							"        # pause for a second to avoid breaching the fair usage policy\n",
							"        time.sleep(1.0)\n",
							"        metric_group_index += 1\n",
							"     \n",
							"    # all the metrics have now been retrieved and merged so convert columns to snake_case and write out the area type's CSV data\n",
							"    df_merged.rename(columns=camel_to_snake, inplace=True)\n",
							"    df_merged[\"date\"]= pd.to_datetime(df_merged[\"date\"])\n",
							"\n",
							"    spark_df = spark.createDataFrame(df_merged)\n",
							"    spark_df.write.mode(\"overwrite\").saveAsTable(f\"c19.{key}\")"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Supplementary Downloads\n",
							"\n",
							"#### UK Population Data\n",
							"\n",
							"We need to download population data for the various geographies (such as UTLA) which are available from GOV.UK.  This will help provide standardised case and death rates between different areas.  \n",
							"\n",
							"As was the case for the COVID-19 data, we snake case the column names."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"df_populations = pd.read_csv(f\"https://coronavirus.data.gov.uk/downloads/supplements/ONS-population_2021-08-05.csv\")\n",
							"df_populations.rename(columns=camel_to_snake, inplace=True)\n",
							"\n",
							"spark_df = spark.createDataFrame(df_populations)\n",
							"spark_df.write.mode(\"overwrite\").saveAsTable(f\"c19.populations\")\n",
							"\n",
							"# preview the data\n",
							"display(df_populations)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### LTLA to UTLA to Region Mappings\r\n",
							"\r\n",
							"This will be used to map different geographical regions types in a hierarchical manner.  "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_ltla_mappings = pd.read_csv(f\"https://raw.githubusercontent.com/happyadam73/c19-notebooks/main/ltla_utla_region_mappings.csv\")\r\n",
							"df_ltla_mappings.rename(columns=camel_to_snake, inplace=True)\r\n",
							"\r\n",
							"spark_df = spark.createDataFrame(df_ltla_mappings)\r\n",
							"spark_df.write.mode(\"overwrite\").saveAsTable(f\"c19.ltla_utla_region_mappings\")\r\n",
							"\r\n",
							"# preview the data\r\n",
							"display(df_ltla_mappings)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Generating Age/Gender Data from JSON based fields\n",
							"\n",
							"You may have noticed in some of the data files generated (like `nation.csv`) that there are some fields that contain a JSON representation of age bracketed data.  For example, the `male_cases` field contains data that looks like this:\n",
							"\n",
							"`[{'age': '30_to_34', 'rate': 12870.5, 'value': 246652}, {'age': '35_to_39', 'rate': 11484.5, 'value': 212805}]`\n",
							"\n",
							"We'd like to parse this data and convert it into more useable data - i.e. have a data-set which provides case numbers by age (group) and gender in addition to the existing key fields (area and date).\n",
							"\n",
							"We're going to transform this data using Pandas Dataframes - first we'll load the `c19.nation` table, perform the transformation and write out the data as a new Spark table.  \n",
							"\n",
							"Most of the parsing of JSON is done using the `pd.concat` and `eval` functions as shown below.  Before we do this, we combine the male cases data with the female cases data and derive an additional gender field."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# read in the full national level data\n",
							"df = spark.table('c19.nation').toPandas()\n",
							"\n",
							"# generate male cases sub-set.  Note we want to filter out all the NaNs, empty strings, and empty lists [] since\n",
							"# these cause issues when using the pd.DataFrame constructor - so filter all records where the JSON is 2 characters or less\n",
							"df_cases_m = df.loc[df[\"male_cases\"].str.len()>2, [\"area_type\",\"area_name\",\"area_code\",\"date\",\"male_cases\"]].rename(columns={\"male_cases\": \"cases\"})\n",
							"df_cases_m[\"gender\"] = \"male\"\n",
							"\n",
							"# now repeat the same process for female cases\n",
							"df_cases_f = df.loc[df[\"female_cases\"].str.len()>2,[\"area_type\",\"area_name\",\"area_code\",\"date\",\"female_cases\"]].rename(columns={\"female_cases\": \"cases\"})\n",
							"df_cases_f[\"gender\"] = \"female\"\n",
							"\n",
							"# now combine the two and replace with a new index\n",
							"df_cases_mf = pd.concat([df_cases_m, df_cases_f], ignore_index=True, sort=False)\n",
							"\n",
							"# now we expand the JSON to create additional records/fields and use the index as the key\n",
							"# this will have a column called 'index' which we can use to join on to df_cases_mf to get the area/gender/date fields\n",
							"df_cases_by_age_index = pd.concat([pd.DataFrame(eval(x)) for x in df_cases_mf[\"cases\"]], keys=df_cases_mf.index).reset_index(level=1, drop=True).reset_index()\n",
							"\n",
							"# now we need to use the index to join onto the previous data frame in order to add the key fields (area, date and gender)\n",
							"df_cases_by_age = df_cases_mf[[\"area_type\",\"area_name\",\"area_code\",\"date\",\"gender\"]].merge(df_cases_by_age_index, left_index=True, right_on=\"index\")\n",
							"\n",
							"# drop the index column\n",
							"df_cases_by_age = df_cases_by_age.drop(columns=[\"index\"])\n",
							"\n",
							"# currently the \"value\" column is cumulative cases, but we'd like new cases only so we need to \n",
							"# sort by date, do a group by and then perform a different on the value column\n",
							"df_cases_by_age = df_cases_by_age.sort_values(by=[\"area_type\",\"area_name\",\"area_code\",\"gender\",\"age\",\"date\"])\n",
							"df_cases_by_age[\"new_cases\"] = df_cases_by_age.groupby([\"area_type\",\"area_name\",\"area_code\",\"gender\",\"age\"])[\"value\"].diff().fillna(0)\n",
							"\n",
							"# rename the values column\n",
							"df_cases_by_age = df_cases_by_age.rename(columns={\"value\": \"cum_cases\"})\n",
							"\n",
							"# finally write the cases by ages data out to CSV\n",
							"#df_cases_by_age.to_csv(f\"cases.csv\", index=False, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
							"\n",
							"spark_df = spark.createDataFrame(df_cases_by_age)\n",
							"spark_df.write.mode(\"overwrite\").saveAsTable(f\"c19.cases\")\n",
							"\n",
							"# preview the data\n",
							"display(df_cases_by_age)"
						],
						"outputs": [],
						"execution_count": null
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Check Row Counts\n",
							"\n",
							"We can run a quick query against the Spark C19 database to check the row counts for each of the tables to ensure the data has loaded."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    SELECT 'National Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.nation\n",
							"    UNION ALL \n",
							"    SELECT 'Summary Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.overview\n",
							"    UNION ALL \n",
							"    SELECT 'NHS Region Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.nhsregion\n",
							"    UNION ALL \n",
							"    SELECT 'Regional Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.region\n",
							"    UNION ALL \n",
							"    SELECT 'UTLA Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.utla\n",
							"    UNION ALL \n",
							"    SELECT 'LTLA Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.ltla\n",
							"    UNION ALL \n",
							"    SELECT 'National Cases by Age and Gender' AS `table`, COUNT(*) AS rows FROM c19.cases\n",
							"    UNION ALL \n",
							"    SELECT 'Age Gender Populations' AS `table`, COUNT(*) AS rows FROM c19.populations\n",
							"    UNION ALL \n",
							"    SELECT 'LTLA/UTLA/Region Mappings' AS `table`, COUNT(*) AS rows FROM c19.ltla_utla_region_mappings\n",
							"    \"\"\"\n",
							")\n",
							"\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 2
					}
				]
			},
			"dependsOn": []
		}
	]
}
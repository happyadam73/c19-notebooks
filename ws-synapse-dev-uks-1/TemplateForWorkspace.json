{
	"$schema": "http://schema.management.azure.com/schemas/2015-01-01/deploymentTemplate.json#",
	"contentVersion": "1.0.0.0",
	"parameters": {
		"workspaceName": {
			"type": "string",
			"metadata": "Workspace name",
			"defaultValue": "ws-synapse-dev-uks-1"
		},
		"ws-synapse-dev-uks-1-WorkspaceDefaultSqlServer_connectionString": {
			"type": "secureString",
			"metadata": "Secure string for 'connectionString' of 'ws-synapse-dev-uks-1-WorkspaceDefaultSqlServer'"
		},
		"ws-synapse-dev-uks-1-WorkspaceDefaultStorage_properties_typeProperties_url": {
			"type": "string",
			"defaultValue": "https://sasynapsedevuks1.dfs.core.windows.net"
		}
	},
	"variables": {
		"workspaceId": "[concat('Microsoft.Synapse/workspaces/', parameters('workspaceName'))]"
	},
	"resources": [
		{
			"name": "[concat(parameters('workspaceName'), '/ws-synapse-dev-uks-1-WorkspaceDefaultSqlServer')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"parameters": {
					"DBName": {
						"type": "String"
					}
				},
				"annotations": [],
				"type": "AzureSqlDW",
				"typeProperties": {
					"connectionString": "[parameters('ws-synapse-dev-uks-1-WorkspaceDefaultSqlServer_connectionString')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/ws-synapse-dev-uks-1-WorkspaceDefaultStorage')]",
			"type": "Microsoft.Synapse/workspaces/linkedServices",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"annotations": [],
				"type": "AzureBlobFS",
				"typeProperties": {
					"url": "[parameters('ws-synapse-dev-uks-1-WorkspaceDefaultStorage_properties_typeProperties_url')]"
				},
				"connectVia": {
					"referenceName": "AutoResolveIntegrationRuntime",
					"type": "IntegrationRuntimeReference"
				}
			},
			"dependsOn": [
				"[concat(variables('workspaceId'), '/integrationRuntimes/AutoResolveIntegrationRuntime')]"
			]
		},
		{
			"name": "[concat(parameters('workspaceName'), '/AutoResolveIntegrationRuntime')]",
			"type": "Microsoft.Synapse/workspaces/integrationRuntimes",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"type": "Managed",
				"typeProperties": {
					"computeProperties": {
						"location": "AutoResolve",
						"dataFlowProperties": {
							"computeType": "General",
							"coreCount": 8,
							"timeToLive": 0
						}
					}
				}
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/COVID 19 Data Viz using Azure Synapse - Part 1 - Extract and Load the Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"bigDataPool": {
					"referenceName": "spkruntime31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/55536dc5-9813-4408-94ca-828f31382219/resourceGroups/rg-synapse-dev-uks-1/providers/Microsoft.Synapse/workspaces/ws-synapse-dev-uks-1/bigDataPools/spkruntime31",
						"name": "spkruntime31",
						"type": "Spark",
						"endpoint": "https://ws-synapse-dev-uks-1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spkruntime31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Data Visualisation of COVID-19 in the UK using Azure Synapse Spark Notebooks \n",
							"## Part 1: Extract and Load the Data\n",
							"\n",
							"This is the first notebook in a 2-part series which explores the UK Gov's COVID-19 dashboard data which is publically available for download via a REST API from gov.uk.  \n",
							"Using PySpark and the GOV.UK COVID-19 SDK we extract the data and load to a set of Spark Sql tables.  \n",
							"\n",
							"In the second notebook of the series, we then use the Seaborn library to visualise the results.\n",
							"\n",
							"The data used in this notebook is publically available and more information can be found here:\n",
							"https://coronavirus.data.gov.uk/details/about-data"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Configuration and Setup\n",
							"\n",
							"The main library we will use to perform the REST API calls to get the GOV.UK COVID-19 data is the SDK published by GOV.UK.  This will handle the various calls, but also pagination as well and populate the data into a Pandas data-frame.\n",
							"\n",
							"Install the library using `pip install uk-covid19` - in Synapse you may want to either add this as a workspace package, or install as a session configuration via an environment.yml file.  Currently this notebook has been tested on version 1.2.2.\n",
							"\n",
							"For more information on the SDK, see: https://pypi.org/project/uk-covid19/\n",
							"\n",
							"> **IMPORTANT**: The uk-covid19 requires a minimum Python version of 3.7 which means the standard runtime (v2.4) will not load this library (Python 3.6)  \n",
							"You need to configure your cluster to use the v3.1 Spark Runtime which provides Python v3.8. "
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"from uk_covid19 import Cov19API  # Use the UK COVID-19 GOV.UK SDK\n",
							"import pandas as pd              # Pandas library for Dataframes\n",
							"import time                      # Need the Sleep function for API calls\n",
							"import csv                       # For CSV generation\n",
							"from datetime import datetime    # Datetime functionality\n",
							"import re                        # Regular Expression for the snake_case function"
						],
						"outputs": [],
						"execution_count": 1
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Camel Case to Snake Case Function for Column Names\n",
							"\n",
							"We're going to use a naming standard for the SQL tables based on Snake Case (lowercase and underscore only).  One reason we might do this is because it's more compliant with various database systems such as HiveQL, Amazon Athena, etc.\n",
							"\n",
							"Unfortunately the column naming used by the GOV.UK API is based on Camel Case (mixed case with no spaces).  So the following function will be used to convert the source column names into snake case which will make importing the data easier."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"def camel_to_snake(name: str) -> str:\n",
							"    \"\"\"\n",
							"    Description: Convert any string to snake case (lower case and _ for spacing)\n",
							"    Args:        name: input string to convert\n",
							"    Returns:     input string converted to snake case \n",
							"    \"\"\"   \n",
							"    name = re.sub(\"(.)([A-Z][a-z]+)\", r\"\\1_\\2\", name)\n",
							"    name = re.sub(\"[,.]\", \"_\", name)   \n",
							"    name = re.sub(\"[+*&%=()?<>!@#$/\\\\\\\\]\", \"\", name)  \n",
							"\n",
							"    return re.sub(\"([a-z0-9])([A-Z])\", r\"\\1_\\2\", name).lower().replace(\" \",\"_\").replace(\"-\",\"_\").replace(\"__\",\"_\").replace(\"__\",\"_\").replace(\"deaths28\",\"deaths_28\")   "
						],
						"outputs": [],
						"execution_count": 2
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Handling Metrics and Area Types in the REST API\n",
							"\n",
							"If you read the documentation provided at https://coronavirus.data.gov.uk/details/download and https://coronavirus.data.gov.uk/details/developers-guide - you'll see that only a maximum of 5 metrics can be requested via the API in addition to the standard (primary key) metrics: Area Type, Name, Code and Date.\n",
							"\n",
							"This means we have to make multiple API requests for groups of metrics since we can't request all in one go.  Additionally the metrics available differ depending on the Area Type you are requesting.\n",
							"\n",
							"The following provides a configuration in the form of a dictionary object which define the metrics available for each area type.  In addition the metrics are broken down into groups of no more than 5 metrics and this will allow us to iterate in a more generic function.  The default metrics are also provided in a separate list object.\n",
							"\n",
							"This notebook is going to retrieve data for all area types, and for most of the metrics available.  However, this is forever changing so as more metrics become available we simply add them to one or more separate metric groups for the appropriate area type."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"default_metrics = [\"areaType\",\n",
							"                   \"areaName\",\n",
							"                   \"areaCode\",\n",
							"                   \"date\"]\n",
							"\n",
							"metrics_by_area_type = {\n",
							"    \"overview\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"newPillarOneTestsByPublishDate\",\"cumPillarOneTestsByPublishDate\"],\n",
							"        [\"newPillarTwoTestsByPublishDate\",\"cumPillarTwoTestsByPublishDate\",\"newPillarThreeTestsByPublishDate\",\"cumPillarThreeTestsByPublishDate\",\"newPillarFourTestsByPublishDate\"],\n",
							"        [\"cumPillarFourTestsByPublishDate\",\"newAdmissions\",\"cumAdmissions\",\"cumTestsByPublishDate\"],\n",
							"        [\"newTestsByPublishDate\",\"covidOccupiedMVBeds\",\"hospitalCases\",\"plannedCapacityByPublishDate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
							"    ],\n",
							"    \"nation\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"maleCases\",\"femaleCases\",\"newPillarOneTestsByPublishDate\",\"cumPillarOneTestsByPublishDate\"],\n",
							"        [\"newPillarTwoTestsByPublishDate\",\"cumPillarTwoTestsByPublishDate\",\"newPillarThreeTestsByPublishDate\",\"cumPillarThreeTestsByPublishDate\"],\n",
							"        [\"newAdmissions\",\"cumAdmissions\",\"cumTestsByPublishDate\"],\n",
							"        [\"newTestsByPublishDate\",\"covidOccupiedMVBeds\",\"hospitalCases\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"],\n",
							"        [\"newPeopleVaccinatedFirstDoseByPublishDate\",\"newPeopleVaccinatedSecondDoseByPublishDate\",\"cumPeopleVaccinatedFirstDoseByPublishDate\",\"cumPeopleVaccinatedSecondDoseByPublishDate\"]\n",
							"    ],\n",
							"    \"region\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
							"    ],  \n",
							"    \"nhsRegion\" : [\n",
							"        [\"newAdmissions\",\"cumAdmissions\",\"covidOccupiedMVBeds\",\"hospitalCases\"]\n",
							"    ],  \n",
							"    \"utla\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
							"    ],\n",
							"    \"ltla\" : [\n",
							"        [\"newCasesByPublishDate\",\"cumCasesByPublishDate\",\"cumCasesByPublishDateRate\",\"newCasesBySpecimenDate\",\"cumCasesBySpecimenDate\"],\n",
							"        [\"cumCasesBySpecimenDateRate\",\"newDeaths28DaysByPublishDate\",\"cumDeaths28DaysByPublishDate\"],\n",
							"        [\"cumDeaths28DaysByPublishDateRate\",\"newDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDate\",\"cumDeaths28DaysByDeathDateRate\"]\n",
							"    ]    \n",
							"}"
						],
						"outputs": [],
						"execution_count": 3
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Extract and Load the Data via REST API by iterating through the metric groups\n",
							"\n",
							"Now we simply iterate through each area type which provides a list of metric groups.  Then we iterate through each metric group (no more than 5 metrics) and combined with the mandatory default metrics we use the SDK to make a REST API call and grab the data into a Pandas dataframe.\n",
							"\n",
							"Unfortunately there is rate limiting implemented for this API as documentated in the fair usage policy: https://coronavirus.data.gov.uk/details/download.  As such we use `time.sleep(1)` to pause for one second between each API call.  So far this has provded sufficient to abide by the fair usage policy.\n",
							"\n",
							"As we iterate through each metric group, we combine all the data together using an outer join merge based on the default metric columns.  In order to avoid Spark data type errors, we convert List based columns to strings.\n",
							"\n",
							"We then convert the column names to snake case, convert the `date` column to datetime, and finally write the data out to the C19 Spark database by converting the Pandas dataframe to a Spark dataframe.\n",
							"\n",
							"**NOTE:** The following code will take several minutes to complete."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"# iterate through each area type (this provides a list of metric groups)\n",
							"# key = areaType, value = list of metric groups (or a list of lists of metrics)\n",
							"for key, value in metrics_by_area_type.items():\n",
							"    \n",
							"    filters = [f\"areaType={key}\"]\n",
							"    \n",
							"    df_merged = pd.DataFrame()\n",
							"    \n",
							"    # iterate through each metric group (at most 5 metrics)\n",
							"    metric_group_index = 1\n",
							"    for metric_group in value:\n",
							"        # we create a dictionary containing the metrics which must also include the default metrics\n",
							"        structure = {}\n",
							"        structure_d = {k: k for k in default_metrics}\n",
							"        structure_m = {k: k for k in metric_group}\n",
							"        structure.update(structure_d)\n",
							"        structure.update(structure_m)\n",
							"\n",
							"        # call the API using the SDK - requires just the Area Type and Metrics requested \n",
							"        api = Cov19API(\n",
							"            filters=filters,\n",
							"            structure=structure\n",
							"        ) \n",
							"        \n",
							"        # grab data as a Pandas dataframe and merge with any previously requested metrics\n",
							"        df = api.get_dataframe() \n",
							"\n",
							"        # convert all list type columns to string\n",
							"        list_col_map = df.applymap(lambda x: isinstance(x, list)).all()\n",
							"        for col in list_col_map.index[list_col_map].tolist():\n",
							"            df[col] = df[col].astype(str)\n",
							"\n",
							"        print(f\"{datetime.now().strftime('%H:%M:%S')} : Area Type: {key} - Metric Group {str(metric_group_index)} - Num records: {len(df)}\", flush=True)\n",
							"        if df_merged.empty:\n",
							"            df_merged = df\n",
							"        else:\n",
							"            df_merged = df_merged.merge(df, on=default_metrics, how=\"outer\")\n",
							"            \n",
							"        # pause for a second to avoid breaching the fair usage policy\n",
							"        time.sleep(1.0)\n",
							"        metric_group_index += 1\n",
							"     \n",
							"    # all the metrics have now been retrieved and merged so convert columns to snake_case and write out the area type's CSV data\n",
							"    df_merged.rename(columns=camel_to_snake, inplace=True)\n",
							"    df_merged[\"date\"]= pd.to_datetime(df_merged[\"date\"])\n",
							"\n",
							"    spark_df = spark.createDataFrame(df_merged)\n",
							"    spark_df.write.mode(\"overwrite\").saveAsTable(f\"c19.{key}\")"
						],
						"outputs": [],
						"execution_count": 4
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Supplementary Downloads\n",
							"\n",
							"#### UK Population Data\n",
							"\n",
							"We need to download population data for the various geographies (such as UTLA) which are available from GOV.UK.  This will help provide standardised case and death rates between different areas.  \n",
							"\n",
							"As was the case for the COVID-19 data, we snake case the column names."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"df_populations = pd.read_csv(f\"https://coronavirus.data.gov.uk/downloads/supplements/ONS-population_2021-08-05.csv\")\n",
							"df_populations.rename(columns=camel_to_snake, inplace=True)\n",
							"\n",
							"spark_df = spark.createDataFrame(df_populations)\n",
							"spark_df.write.mode(\"overwrite\").saveAsTable(f\"c19.populations\")\n",
							"\n",
							"# preview the data\n",
							"display(df_populations)"
						],
						"outputs": [],
						"execution_count": 5
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"#### LTLA to UTLA to Region Mappings\r\n",
							"\r\n",
							"This will be used to map different geographical regions types in a hierarchical manner.  "
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"jupyter": {
								"source_hidden": false,
								"outputs_hidden": false
							},
							"nteract": {
								"transient": {
									"deleting": false
								}
							},
							"collapsed": false
						},
						"source": [
							"df_ltla_mappings = pd.read_csv(f\"https://raw.githubusercontent.com/happyadam73/c19-notebooks/main/ltla_utla_region_mappings.csv\")\r\n",
							"df_ltla_mappings.rename(columns=camel_to_snake, inplace=True)\r\n",
							"\r\n",
							"spark_df = spark.createDataFrame(df_ltla_mappings)\r\n",
							"spark_df.write.mode(\"overwrite\").saveAsTable(f\"c19.ltla_utla_region_mappings\")\r\n",
							"\r\n",
							"# preview the data\r\n",
							"display(df_ltla_mappings)"
						],
						"outputs": [],
						"execution_count": 6
					},
					{
						"cell_type": "markdown",
						"metadata": {
							"nteract": {
								"transient": {
									"deleting": false
								}
							}
						},
						"source": [
							"### Generating Age/Gender Data from JSON based fields\n",
							"\n",
							"You may have noticed in some of the data files generated (like `nation.csv`) that there are some fields that contain a JSON representation of age bracketed data.  For example, the `male_cases` field contains data that looks like this:\n",
							"\n",
							"`[{'age': '30_to_34', 'rate': 12870.5, 'value': 246652}, {'age': '35_to_39', 'rate': 11484.5, 'value': 212805}]`\n",
							"\n",
							"We'd like to parse this data and convert it into more useable data - i.e. have a data-set which provides case numbers by age (group) and gender in addition to the existing key fields (area and date).\n",
							"\n",
							"We're going to transform this data using Pandas Dataframes - first we'll load the `c19.nation` table, perform the transformation and write out the data as a new Spark table.  \n",
							"\n",
							"Most of the parsing of JSON is done using the `pd.concat` and `eval` functions as shown below.  Before we do this, we combine the male cases data with the female cases data and derive an additional gender field."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"# read in the full national level data\n",
							"df = spark.table('c19.nation').toPandas()\n",
							"\n",
							"# generate male cases sub-set.  Note we want to filter out all the NaNs, empty strings, and empty lists [] since\n",
							"# these cause issues when using the pd.DataFrame constructor - so filter all records where the JSON is 2 characters or less\n",
							"df_cases_m = df.loc[df[\"male_cases\"].str.len()>2, [\"area_type\",\"area_name\",\"area_code\",\"date\",\"male_cases\"]].rename(columns={\"male_cases\": \"cases\"})\n",
							"df_cases_m[\"gender\"] = \"male\"\n",
							"\n",
							"# now repeat the same process for female cases\n",
							"df_cases_f = df.loc[df[\"female_cases\"].str.len()>2,[\"area_type\",\"area_name\",\"area_code\",\"date\",\"female_cases\"]].rename(columns={\"female_cases\": \"cases\"})\n",
							"df_cases_f[\"gender\"] = \"female\"\n",
							"\n",
							"# now combine the two and replace with a new index\n",
							"df_cases_mf = pd.concat([df_cases_m, df_cases_f], ignore_index=True, sort=False)\n",
							"\n",
							"# now we expand the JSON to create additional records/fields and use the index as the key\n",
							"# this will have a column called 'index' which we can use to join on to df_cases_mf to get the area/gender/date fields\n",
							"df_cases_by_age_index = pd.concat([pd.DataFrame(eval(x)) for x in df_cases_mf[\"cases\"]], keys=df_cases_mf.index).reset_index(level=1, drop=True).reset_index()\n",
							"\n",
							"# now we need to use the index to join onto the previous data frame in order to add the key fields (area, date and gender)\n",
							"df_cases_by_age = df_cases_mf[[\"area_type\",\"area_name\",\"area_code\",\"date\",\"gender\"]].merge(df_cases_by_age_index, left_index=True, right_on=\"index\")\n",
							"\n",
							"# drop the index column\n",
							"df_cases_by_age = df_cases_by_age.drop(columns=[\"index\"])\n",
							"\n",
							"# currently the \"value\" column is cumulative cases, but we'd like new cases only so we need to \n",
							"# sort by date, do a group by and then perform a different on the value column\n",
							"df_cases_by_age = df_cases_by_age.sort_values(by=[\"area_type\",\"area_name\",\"area_code\",\"gender\",\"age\",\"date\"])\n",
							"df_cases_by_age[\"new_cases\"] = df_cases_by_age.groupby([\"area_type\",\"area_name\",\"area_code\",\"gender\",\"age\"])[\"value\"].diff().fillna(0)\n",
							"\n",
							"# rename the values column\n",
							"df_cases_by_age = df_cases_by_age.rename(columns={\"value\": \"cum_cases\"})\n",
							"\n",
							"# finally write the cases by ages data out to CSV\n",
							"#df_cases_by_age.to_csv(f\"cases.csv\", index=False, quotechar='\"', quoting=csv.QUOTE_NONNUMERIC)\n",
							"\n",
							"spark_df = spark.createDataFrame(df_cases_by_age)\n",
							"spark_df.write.mode(\"overwrite\").saveAsTable(f\"c19.cases\")\n",
							"\n",
							"# preview the data\n",
							"display(df_cases_by_age)"
						],
						"outputs": [],
						"execution_count": 7
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Check Row Counts\n",
							"\n",
							"We can run a quick query against the Spark C19 database to check the row counts for each of the tables to ensure the data has loaded."
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    SELECT 'National Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.nation\n",
							"    UNION ALL \n",
							"    SELECT 'Summary Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.overview\n",
							"    UNION ALL \n",
							"    SELECT 'NHS Region Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.nhsregion\n",
							"    UNION ALL \n",
							"    SELECT 'Regional Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.region\n",
							"    UNION ALL \n",
							"    SELECT 'UTLA Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.utla\n",
							"    UNION ALL \n",
							"    SELECT 'LTLA Daily Metrics' AS `table`, COUNT(*) AS rows FROM c19.ltla\n",
							"    UNION ALL \n",
							"    SELECT 'National Cases by Age and Gender' AS `table`, COUNT(*) AS rows FROM c19.cases\n",
							"    UNION ALL \n",
							"    SELECT 'Age Gender Populations' AS `table`, COUNT(*) AS rows FROM c19.populations\n",
							"    UNION ALL \n",
							"    SELECT 'LTLA/UTLA/Region Mappings' AS `table`, COUNT(*) AS rows FROM c19.ltla_utla_region_mappings\n",
							"    \"\"\"\n",
							")\n",
							"\n",
							"display(df)"
						],
						"outputs": [],
						"execution_count": 8
					}
				]
			},
			"dependsOn": []
		},
		{
			"name": "[concat(parameters('workspaceName'), '/COVID 19 Data Viz using Azure Synapse - Part 2 - Visualise the Data')]",
			"type": "Microsoft.Synapse/workspaces/notebooks",
			"apiVersion": "2019-06-01-preview",
			"properties": {
				"bigDataPool": {
					"referenceName": "spkruntime31",
					"type": "BigDataPoolReference"
				},
				"sessionProperties": {
					"driverMemory": "28g",
					"driverCores": 4,
					"executorMemory": "28g",
					"executorCores": 4,
					"numExecutors": 2,
					"conf": {
						"spark.dynamicAllocation.enabled": "false",
						"spark.dynamicAllocation.minExecutors": "2",
						"spark.dynamicAllocation.maxExecutors": "2"
					}
				},
				"metadata": {
					"saveOutput": true,
					"kernelspec": {
						"name": "synapse_pyspark",
						"display_name": "Synapse PySpark"
					},
					"language_info": {
						"name": "python"
					},
					"a365ComputeOptions": {
						"id": "/subscriptions/55536dc5-9813-4408-94ca-828f31382219/resourceGroups/rg-synapse-dev-uks-1/providers/Microsoft.Synapse/workspaces/ws-synapse-dev-uks-1/bigDataPools/spkruntime31",
						"name": "spkruntime31",
						"type": "Spark",
						"endpoint": "https://ws-synapse-dev-uks-1.dev.azuresynapse.net/livyApi/versions/2019-11-01-preview/sparkPools/spkruntime31",
						"auth": {
							"type": "AAD",
							"authResource": "https://dev.azuresynapse.net"
						},
						"sparkVersion": "3.1",
						"nodeCount": 3,
						"cores": 4,
						"memory": 28
					},
					"sessionKeepAliveTimeout": 30
				},
				"cells": [
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"# Data Visualisation of COVID-19 in the UK using Azure Synapse and Seaborn Libraries \n",
							"## Part 2: Visualise the Data\n",
							"\n",
							"This notebook explores the UK Gov's COVID-19 dashboard data.  \n",
							"\n",
							"The dashboard data is made up of several tables which consist of different metrics (such as positive cases, deaths and hospital admissions) broken down at different geographical levels - \n",
							"some metrics are available at lower or upper tier local authority (LTLA/UTLA), some at Region and NHS Region and some nationally or overall within the UK. \n",
							"\n",
							"We will explore several of these tables to produce queries and visualisations of various metrics at different geography levels.  \n",
							"\n",
							"The data used in this notebook is publically available and more information can be found here:\n",
							"https://coronavirus.data.gov.uk/details/about-data"
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Configuration and Setup\n",
							"\n",
							"We also import the Seaborn and Plotly express libraries for use in data visualisation.\n",
							"\n",
							"These should already be installed with the Apache Spark runtime."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import seaborn as sns\n",
							"import plotly.express as px\n",
							"import plotly\n",
							"import pandas as pd\n",
							"import matplotlib.pyplot as plt"
						],
						"outputs": [],
						"execution_count": 38
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Querying Basics: England Positive Cases by Date\n",
							"\n",
							"Let's start of with a simple example.  We're going to get all the positive test cases that occurred in England from the start of the Covid pandemic, and plot these against the published date."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"        SELECT date, new_cases_by_publish_date\n",
							"        FROM c19.nation\n",
							"        WHERE area_name = 'England' \n",
							"        ORDER BY date    \n",
							"    \"\"\"\n",
							").toPandas()\n",
							"\n",
							"df.head()"
						],
						"outputs": [],
						"execution_count": 39
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"Pretty simple query - select the date and positive cases, filter this by England (since this table contains other UK nations), and then order by the date field.  Let's generate a line plot and see how this looks ..."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"sns.set(rc={'figure.figsize':(15,8)})\n",
							"g = sns.lineplot(data=df, x=\"date\", y=\"new_cases_by_publish_date\").set(title=\"England Positive Cases by Date\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 40
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Introducing Windowed functions and CTEs\n",
							"\n",
							"Hmmmm, OK - so this looks OK but it's not a smooth line.  This is probably due to the nature of reporting during the week when lower numbers are recorded at the weekend.  So let's fix that and generate a 7 day moving average - to do this we use an aggregated Window function as shown below - it generates a SUM of new cases over the last 7 days (current row = 0).  \n",
							"\n",
							"``` mysql\n",
							"SUM(new_cases_by_publish_date) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW)\n",
							"```\n",
							"\n",
							"But let's also modify the query to show us the last 365 days.  Now we could use a hard-coded date filter, but that means everytime you run the query, you need to change the values.  So let's generate a derived field (```num_days_ago```) and we can filter on this.  Again we use window function which is ordered by date - but this time in descending order, and we simply generate a sequential value:\n",
							"\n",
							"``` mysql\n",
							"ROW_NUMBER() OVER (ORDER BY date DESC)\n",
							"```\n",
							"\n",
							"So one more thing we need to do.  If we want to filter by a Window functioned value that we've derived, we essentially need to somehow alias it with a new column name, and then filter in a separate expression.  We do this using a Common Table Expression (CTE) using the ```WITH``` statement.\n",
							"\n",
							"``` mysql\n",
							"WITH cte AS (\n",
							"    ...\n",
							")\n",
							"SELECT * FROM cte\n",
							"```\n",
							"\n",
							"Now let's put all this in action and try to plot the data again."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    WITH england_cases AS (\n",
							"        SELECT \n",
							"            date, \n",
							"            SUM(new_cases_by_publish_date) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS positive_cases_7d_rolling_sum,\n",
							"            ROW_NUMBER() OVER (ORDER BY date DESC) AS num_days_ago\n",
							"        FROM c19.nation\n",
							"        WHERE area_name = 'England'\n",
							"    )\n",
							"    SELECT\n",
							"        date,\n",
							"        positive_cases_7d_rolling_sum\n",
							"    FROM england_cases\n",
							"    WHERE num_days_ago <= 365\n",
							"    \"\"\"\n",
							").toPandas()\n",
							"\n",
							"sns.set(rc={'figure.figsize':(15,8)})\n",
							"g = sns.lineplot(data=df, x=\"date\", y=\"positive_cases_7d_rolling_sum\").set(title=\"7-day Total England Positive Cases by Date\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 41
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"That's much better.  We'll use the CTEs and Window functions in the following queries.  Let's move on to a slightly more complex query."
						]
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Query Table Joins: 7-day average deaths per 1 Million Population broken down by Region\n",
							"\n",
							"We want to look at deaths by region.  Again we use a window function to produce a 7 day rolling average to smooth out lower reported deaths at the weekend.  \n",
							"However, different regions have different populations so to compare we want to standardise the average deaths by the region's population.\n",
							"\n",
							"For this we'll look at the ```c19.populations``` table which provides populations by age (and all ages) at LSOA geographical area.  \n",
							"In order to get population by region, we need to be able to map LSOAs to Regions and we can do this using the ```c19.ltla_utla_region_mappings``` table.\n",
							"\n",
							"So we join the two tables and simply SUM the ```all_ages``` population to get regional populations like this:\n",
							"\n",
							"```mysql\n",
							"    SELECT\n",
							"        lsoa_to_rgn.rgn_name,\n",
							"        SUM(pop_lsoa.all_ages) AS total_population   \n",
							"    FROM \n",
							"        c19.populations pop_lsoa\n",
							"        INNER JOIN c19.ltla_utla_region_mappings lsoa_to_rgn\n",
							"            ON pop_lsoa.code = lsoa_to_rgn.lsoa_code\n",
							"    GROUP BY     \n",
							"        lsoa_to_rgn.rgn_name\n",
							"```\n",
							"\n",
							"We can then apply this to what we did previously using CTEs, Window functions (for the 7 day average death) to get the breakdown we need.  For this we'll limit the timescales to the last 90 days. \n",
							"\n",
							"However, because we're now introducing an extra level of aggregation (in this case Region), then we need to apply that extra level of breakdown to the Windowed function calls and we do this using the ```PARTITION BY``` keyword.   \n",
							"For example, generating a sequential number for the date (```num_days_ago```) which is the same sequence for each region, we can do the following:\n",
							"\n",
							"```mysql \n",
							"ROW_NUMBER() OVER (PARTITION BY area_name ORDER BY date DESC)\n",
							"```"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    WITH all_populations AS (\n",
							"        SELECT \n",
							"            area_code,\n",
							"            population\n",
							"        FROM c19.populations\n",
							"        WHERE category = 'ALL'\n",
							"    ), region_7d_deaths AS (\n",
							"        SELECT\n",
							"            area_code AS region_code,\n",
							"            area_name AS region_name,\n",
							"            date AS publish_date,\n",
							"            AVG(1.0 * new_deaths_28_days_by_death_date) OVER (PARTITION BY area_name ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS deaths_7d_rolling_avg,\n",
							"            ROW_NUMBER() OVER (PARTITION BY area_name ORDER BY date DESC) AS num_days_ago\n",
							"        FROM c19.region\n",
							"    )\n",
							"    SELECT \n",
							"        rgn.publish_date,\n",
							"        rgn.region_name,\n",
							"        (rgn.deaths_7d_rolling_avg * 1000000.0)/pop.population AS deaths_7d_avg_per_1M_population  \n",
							"    FROM \n",
							"        region_7d_deaths rgn\n",
							"        INNER JOIN all_populations pop\n",
							"            ON rgn.region_code = pop.area_code\n",
							"    WHERE \n",
							"        rgn.num_days_ago <= 90\n",
							"    ORDER BY\n",
							"        rgn.publish_date,\n",
							"        rgn.region_name    \n",
							"    \"\"\"\n",
							").toPandas()\n",
							"\n",
							"df.head()"
						],
						"outputs": [],
						"execution_count": 42
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"Let's plot how the death rates vary by region over the last 90 days"
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"sns.set(rc={'figure.figsize':(15,8)})\n",
							"g = sns.lineplot(data=df, x=\"publish_date\", y=\"deaths_7d_avg_per_1M_population\", hue=\"region_name\", style=\"region_name\").set(title=\"7-day average deaths per 1 Million Population broken down by Region\")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 43
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### England 7-day rolling sum positive cases by Age Group and Gender (over last 60 days)\n",
							"\n",
							"Let's do a slightly different breakdown - this time positive cases by Age Group and Gender.  The ```c19.nation``` table provides cases by different age groups and genders.\n",
							"\n",
							"Again we use a window function to produce a 7 day rolling sum to smooth out lower reported cases at the weekend and we'll filter the dates to only show the last 60 days.\n",
							"\n",
							"We can then use Seaborn's ```relplot``` function to provide line charts in a grid (faceted) - so the X, Y axis is similar to before: cases by date, but now a separate graph is generated per age group providing a rapid way to visualise the data.  In addition we use the colour (hue) parameter to provide male and female lines on the same graph.\n",
							"\n",
							"Because each graph's X axis is small, using the actual date looks cluttered and unreadable.  So for this example, we use the ```weekofyear``` function to convert the date to a single number (week of the year) which looks better on the graphs.\n",
							"\n",
							"Note that the 0-4 and 5-9 age groups need to be converted to include a leading 0 digit so that the ordering of graphs is in the correct age order.\n",
							""
						]
					},
					{
						"cell_type": "code",
						"metadata": {
							"collapsed": false
						},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    WITH cases_by_age AS (\n",
							"        SELECT\n",
							"            weekofyear(date) AS week_of_year,\n",
							"            gender,\n",
							"            CASE \n",
							"                WHEN age = '0_to_4' THEN '00_to_04'\n",
							"                WHEN age = '5_to_9' THEN '05_to_09'\n",
							"                ELSE age \n",
							"            END AS age,\n",
							"            SUM(new_cases) OVER (PARTITION BY gender, age ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS cases_7d_rolling_sum,\n",
							"            ROW_NUMBER() OVER (PARTITION BY gender, age ORDER BY date DESC) AS num_days_ago\n",
							"        FROM c19.cases\n",
							"        WHERE area_name = 'England'\n",
							"    )\n",
							"    SELECT\n",
							"        week_of_year,\n",
							"        gender,\n",
							"        age,\n",
							"        cases_7d_rolling_sum\n",
							"    FROM cases_by_age\n",
							"    WHERE\n",
							"        num_days_ago <= 60\n",
							"    ORDER BY\n",
							"        gender,\n",
							"        age,\n",
							"        week_of_year\n",
							"    \"\"\"\n",
							").toPandas()\n",
							"\n",
							"\n",
							"g = sns.relplot(\n",
							"    data=df, x=\"week_of_year\", y=\"cases_7d_rolling_sum\", \n",
							"    col=\"age\", hue=\"gender\", hue_order=[\"male\",\"female\"], \n",
							"    col_wrap=4, kind=\"line\"\n",
							")\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 44
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Latest 7-day Positive Cases and Deaths by Region-UTLA hierarchy\n",
							"\n",
							"In this example we'll use a Plotly Treemap which is interactive and let's us drilldown from Region to UTLA level.  We visualise the positive cases by size, and deaths by colour.\n",
							"\n",
							"First we'll need the number of cases/deaths at UTLA level which we can find in the ```c19.utla``` table.  \n",
							"Additionally we'll take a similar approach in the previous example to get population by UTLA, as well as generating a mapping between UTLA and Region using the ```c19.ltla_utla_region_mappings``` table.\n",
							"\n",
							"We only ever want the latest figures (based on the latest 7 day window), so we filter the results by the latest date using a sub-query which looks like this:\n",
							"\n",
							"```mysql\n",
							"WHERE date = (SELECT MAX(date) FROM c19.utla)\n",
							"```\n",
							"\n",
							"The following query give us what we need:  Region, UTLA, Positive Cases (7-day rolling sum per 100,000 population) and Deaths (7-day rolling average per 1 million population)."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    WITH utla_to_region AS (\n",
							"        SELECT DISTINCT \n",
							"            utla_code,\n",
							"            region_name\n",
							"        FROM c19.ltla_utla_region_mappings   \n",
							"    ), utla_populations AS (    \n",
							"        SELECT \n",
							"            area_code,\n",
							"            population\n",
							"        FROM c19.populations\n",
							"        WHERE category = 'ALL'\n",
							"    ), utla_metrics AS (\n",
							"        SELECT\n",
							"            utla.date,\n",
							"            utla.area_code AS utla_code,\n",
							"            utla.area_name AS utla_name,\n",
							"            utla_to_rgn.region_name,\n",
							"            SUM(1.0 * utla.new_cases_by_publish_date) OVER (PARTITION BY utla.area_code ORDER BY utla.date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS positive_cases_7d_rolling_sum,\n",
							"            AVG(1.0 * utla.new_deaths_28_days_by_publish_date) OVER (PARTITION BY utla.area_code ORDER BY utla.date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS deaths_7d_rolling_avg\n",
							"        FROM \n",
							"            c19.utla utla\n",
							"            INNER JOIN utla_to_region utla_to_rgn\n",
							"                ON utla.area_code = utla_to_rgn.utla_code    \n",
							"    )\n",
							"    SELECT\n",
							"        utla.region_name,\n",
							"        utla.utla_name,\n",
							"        CAST((utla.positive_cases_7d_rolling_sum * 100000.0)/pop.population AS INT) AS positive_cases_7d_sum_per_100K_population,\n",
							"        CAST((utla.deaths_7d_rolling_avg * 1000000.0)/pop.population AS REAL) AS deaths_7d_avg_per_1M_population\n",
							"    FROM \n",
							"        utla_metrics utla\n",
							"        INNER JOIN utla_populations pop\n",
							"            ON utla.utla_code = pop.area_code\n",
							"    WHERE utla.date = (SELECT MAX(date) FROM utla_metrics)\n",
							"    ORDER BY positive_cases_7d_sum_per_100K_population DESC\n",
							"    \"\"\"\n",
							").toPandas()\n",
							"\n",
							"# Since we've used an ORDER BY, the following will show the Top 10 UTLAs for Positive Cases\n",
							"df.head(10)"
						],
						"outputs": [],
						"execution_count": 45
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"#### Using the Plotly Express Treemap\n",
							"\n",
							"To create the treemap, we use Plotly Express.  Using the path parameter we can setup the hierachy between Region and UTLA, and then specify what the treemap size is based on and what the colours are based on.  Once the chart is generated, you can zoom in and out of specific regions.  Note that Wales doesn't have values for the deaths metric so there is no colour codes."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"fig = px.treemap(df, path=[px.Constant(\"UK\"),'region_name', 'utla_name'], values='positive_cases_7d_sum_per_100K_population',\n",
							"                  color='deaths_7d_avg_per_1M_population',\n",
							"                  color_continuous_scale='reds')\n",
							"fig.data[0].textinfo = 'label+text+value'\n",
							"fig.update_layout(margin = dict(t=20, l=0, r=0, b=20), autosize=False,width=1200,height=700)\n",
							"#fig.show()\n",
							"\n",
							"# create an html document that embeds the Plotly plot\n",
							"h = plotly.offline.plot(fig, output_type='div')\n",
							"\n",
							"# display this html\n",
							"displayHTML(h)"
						],
						"outputs": [],
						"execution_count": 46
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Digging Deeper: Latest 30-day time-series of Top 5 UTLAs by Positive Cases for all Regions\n",
							"\n",
							"Let's go further.  This time we shall produce the latest 30-day time-series for all Regions.  Each of these charts will show the top 5 UTLAs for the region in terms of 7-day rolling sum per 100,000 population positive COVID-19 cases.\n",
							"\n",
							"There's nothing really new in this example, apart from the complexity.  To make this easier to manage, we break the full query down into a set of common table expressions (CTEs).  Essentially we need to perform 2 separate rankings.  One set of rankings is used to generate the top 5 UTLAs and we do this based on the latest date.  The other is ranked by date so we can filter out only the latest 30 days.  We then join these together.  It's pretty easy to modify the query below to change the timescale (to more or less than 30 days) as well as how much UTLAs are included in each region's rankings.\n",
							"\n",
							"Finally we use Seaborn's ```relplot``` to produce a 3 column grid of time-series plots, one for each region.  Note the x labels become illegible because of the size of the graphs so we rotate these to make them clearer to read."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    \n",
							"    WITH utla_to_region AS (\n",
							"        SELECT DISTINCT \n",
							"            utla_code,\n",
							"            region_name\n",
							"        FROM c19.ltla_utla_region_mappings   \n",
							"    ), populations_by_utla AS (    \n",
							"        SELECT \n",
							"            utla_to_rgn.region_name,\n",
							"            utla_to_rgn.utla_code,\n",
							"            pop.population AS total_population\n",
							"        FROM \n",
							"            c19.populations pop\n",
							"            INNER JOIN utla_to_region utla_to_rgn\n",
							"                ON pop.area_code = utla_to_rgn.utla_code\n",
							"        WHERE pop.category = 'ALL'\n",
							"    ), pos_7d_cases_by_utla_and_date AS (\n",
							"        SELECT\n",
							"            area_code AS utla_code,\n",
							"            area_name AS utla_name,\n",
							"            date AS publish_date,\n",
							"            SUM(1.0 * new_cases_by_publish_date) OVER (PARTITION BY area_code ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS positive_cases_7d_rolling_sum\n",
							"        FROM c19.utla\n",
							"    ), pos_7d_cases_per_pop_by_utla_and_date AS (\n",
							"        SELECT\n",
							"            pop.region_name,\n",
							"            utla.utla_code,\n",
							"            utla.utla_name,\n",
							"            utla.publish_date,\n",
							"            CAST((utla.positive_cases_7d_rolling_sum * 100000.0)/pop.total_population AS INTEGER) AS cases_7d_rate_per_100k_population\n",
							"        FROM \n",
							"            pos_7d_cases_by_utla_and_date utla\n",
							"            INNER JOIN populations_by_utla pop\n",
							"                ON utla.utla_code = pop.utla_code\n",
							"    ), pos_7d_cases_per_pop_by_utla_and_date_with_date_rank AS (\n",
							"        SELECT\n",
							"            region_name,\n",
							"            utla_code,\n",
							"            utla_name,\n",
							"            publish_date,\n",
							"            cases_7d_rate_per_100k_population,\n",
							"            ROW_NUMBER() OVER (PARTITION BY utla_code ORDER BY publish_date DESC) AS num_days_ago\n",
							"        FROM pos_7d_cases_per_pop_by_utla_and_date \n",
							"    ), pos_7d_cases_per_pop_by_utla_and_date_with_case_rank AS (\n",
							"        SELECT\n",
							"            region_name,\n",
							"            utla_code,\n",
							"            utla_name,\n",
							"            publish_date,\n",
							"            ROW_NUMBER() OVER (PARTITION BY region_name ORDER BY cases_7d_rate_per_100k_population DESC) AS positive_cases_7d_rolling_sum_rank\n",
							"        FROM pos_7d_cases_per_pop_by_utla_and_date \n",
							"        WHERE \n",
							"            publish_date = (SELECT MAX(publish_date) FROM pos_7d_cases_per_pop_by_utla_and_date)\n",
							"    )\n",
							"    SELECT\n",
							"        r1.region_name,\n",
							"        r1.utla_name,\n",
							"        r1.publish_date,\n",
							"        r1.cases_7d_rate_per_100k_population\n",
							"    FROM \n",
							"        pos_7d_cases_per_pop_by_utla_and_date_with_date_rank r1\n",
							"        INNER JOIN pos_7d_cases_per_pop_by_utla_and_date_with_case_rank r2\n",
							"            ON r1.utla_code = r2.utla_code\n",
							"    WHERE\n",
							"        r1.num_days_ago <= 30\n",
							"    AND r2.positive_cases_7d_rolling_sum_rank <= 5\n",
							"    ORDER BY\n",
							"            r1.region_name,\n",
							"            r2.positive_cases_7d_rolling_sum_rank,\n",
							"            r1.publish_date\n",
							"    \"\"\"\n",
							").toPandas()\n",
							"\n",
							"g = sns.relplot(\n",
							"    data=df, x=\"publish_date\", y=\"cases_7d_rate_per_100k_population\", \n",
							"    hue=\"utla_name\", style=\"utla_name\", col=\"region_name\", \n",
							"    col_wrap=3, kind=\"line\", palette=\"Set1\"\n",
							")\n",
							"\n",
							"# rotate x labels so they're legible\n",
							"tx = g.set_xticklabels(rotation=30)\n",
							"\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 47
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Hospital Metrics over the last 60 days by NHS Region\n",
							"\n",
							"So far we've looked at positive cases and deaths.  Now we explore some hospital metrics: number of admissions, number of patients in hospital and patients on mechanical ventilation.\n",
							"\n",
							"This data is available at NHS Region level using the ```c19.nhsregion``` table.\n",
							"\n",
							"In order to use Seaborn's ```relplot``` function we need to move away from wide format data.  This refers to the three metrics as being three separate columns.  \n",
							"In order to plot multiple series, we need to convert to long format - that is one row per metric.\n",
							"\n",
							"This is pretty straightforward using SQL - we simply have a ```SELECT``` statement for each metric and use ```UNION ALL``` to stitch them together into one output.  \n",
							"We have one column to define which metric (```metric_type```), and one column for the metric's value (```metric_value```).  \n",
							"We can then pass these to the ```relplot``` function to have have a time-series with all 3 metrics.  We then produce separate charts for each of the NHS Regions."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    WITH nhs_region_metrics_wide_format AS (\n",
							"        SELECT\n",
							"            area_name AS nhs_region,\n",
							"            date,\n",
							"            new_admissions,\n",
							"            covid_occupied_mv_beds,\n",
							"            hospital_cases,\n",
							"            ROW_NUMBER() OVER (PARTITION BY area_name ORDER BY date DESC) AS num_days_ago\n",
							"        FROM c19.nhsregion\n",
							"    ), nhs_region_metrics AS (\n",
							"        SELECT nhs_region, date, num_days_ago, new_admissions AS metric_value, 'New Admissions' AS metric_type\n",
							"        FROM nhs_region_metrics_wide_format\n",
							"            UNION ALL\n",
							"        SELECT nhs_region, date, num_days_ago, covid_occupied_mv_beds AS metric_value, 'On ventilation' AS metric_type\n",
							"        FROM nhs_region_metrics_wide_format\n",
							"            UNION ALL\n",
							"        SELECT nhs_region, date, num_days_ago, hospital_cases AS metric_value, 'In Hospital' AS metric_type\n",
							"        FROM nhs_region_metrics_wide_format\n",
							"    )\n",
							"    SELECT \n",
							"        nhs_region, \n",
							"        date, \n",
							"        metric_value, \n",
							"        metric_type\n",
							"    FROM nhs_region_metrics\n",
							"    WHERE num_days_ago <= 60\n",
							"    \"\"\"\n",
							").toPandas()\n",
							"\n",
							"g = sns.relplot(\n",
							"    data=df, x=\"date\", y=\"metric_value\", \n",
							"    col=\"nhs_region\", style=\"metric_type\",\n",
							"    col_wrap=3, kind=\"line\", palette=\"Set1\"\n",
							")\n",
							"\n",
							"# rotate x labels so they're legible\n",
							"tx = g.set_xticklabels(rotation=30)\n",
							"\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 49
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### England 7-day average Cases, Deaths, Vaccinations and Hospital Metrics over the last 120 days \n",
							"\n",
							"Similar to the previous example, we can show separate plots for each of the metrics available at England level using the ```c19.nation``` table.\n",
							"\n",
							"Again we convert the wide format data into long format using the same ```UNION ALL``` technique as we did last time.\n",
							"\n",
							"One difference this time is that we really need the separate plots to have a separate Y-axis scale, so for this we use the ```facet_kws``` parameter and specify that the Y-axis is not shared."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.sql(\n",
							"    \"\"\"\n",
							"    WITH england_metrics AS (\n",
							"        SELECT\n",
							"            date,\n",
							"            new_cases_by_publish_date,\n",
							"            new_admissions,\n",
							"            covid_occupied_mv_beds,\n",
							"            hospital_cases,\n",
							"            new_deaths_28_days_by_publish_date,\n",
							"            new_tests_by_publish_date,\n",
							"            new_people_vaccinated_first_dose_by_publish_date,\n",
							"            new_people_vaccinated_second_dose_by_publish_date,\n",
							"            ROW_NUMBER() OVER (ORDER BY date DESC) AS num_days_ago    \n",
							"        FROM \n",
							"            c19.nation\n",
							"        WHERE \n",
							"            area_name = 'England'\n",
							"    ), england_metrics_7d_avg AS (\n",
							"        SELECT\n",
							"            date,\n",
							"            AVG(new_cases_by_publish_date) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS cases_7d_avg,\n",
							"            AVG(new_admissions) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS admissions_7d_avg,\n",
							"            AVG(covid_occupied_mv_beds) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS on_ventilators_7d_avg,\n",
							"            AVG(hospital_cases) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS in_hospital_7d_avg,\n",
							"            AVG(new_deaths_28_days_by_publish_date) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS deaths_7d_avg,\n",
							"            AVG(new_tests_by_publish_date) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS tests_7d_avg,\n",
							"            AVG(new_people_vaccinated_first_dose_by_publish_date) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS first_dose_7d_avg,\n",
							"            AVG(new_people_vaccinated_second_dose_by_publish_date) OVER (ORDER BY date ROWS BETWEEN 6 PRECEDING AND CURRENT ROW) AS second_dose_7d_avg,\n",
							"            ROW_NUMBER() OVER (ORDER BY date DESC) AS num_days_ago    \n",
							"        FROM \n",
							"            england_metrics\n",
							"    ), england_metrics_pivot AS (\n",
							"            SELECT date, num_days_ago, cases_7d_avg AS metric_value, '01 - Cases' AS metric_type\n",
							"            FROM england_metrics_7d_avg\n",
							"                UNION ALL\n",
							"            SELECT date, num_days_ago, deaths_7d_avg AS metric_value, '02 - Deaths' AS metric_type\n",
							"            FROM england_metrics_7d_avg\n",
							"                UNION ALL\n",
							"            SELECT date, num_days_ago, first_dose_7d_avg AS metric_value, '03 - First Dose Vaccine' AS metric_type\n",
							"            FROM england_metrics_7d_avg    \n",
							"                UNION ALL\n",
							"            SELECT date, num_days_ago, second_dose_7d_avg AS metric_value, '04 - Second Dose Vaccine' AS metric_type\n",
							"            FROM england_metrics_7d_avg                 \n",
							"                UNION ALL\n",
							"            SELECT date, num_days_ago, admissions_7d_avg AS metric_value, '05 - Admissions' AS metric_type\n",
							"            FROM england_metrics_7d_avg\n",
							"                UNION ALL\n",
							"            SELECT date, num_days_ago, on_ventilators_7d_avg AS metric_value, '06 - On Ventilation' AS metric_type\n",
							"            FROM england_metrics_7d_avg\n",
							"                UNION ALL\n",
							"            SELECT date, num_days_ago, in_hospital_7d_avg AS metric_value, '07 - In Hospital' AS metric_type\n",
							"            FROM england_metrics_7d_avg            \n",
							"                UNION ALL\n",
							"            SELECT date, num_days_ago, tests_7d_avg AS metric_value, '08 - Tests' AS metric_type\n",
							"            FROM england_metrics_7d_avg\n",
							"    )    \n",
							"    SELECT \n",
							"        date, \n",
							"        metric_value, \n",
							"        metric_type\n",
							"    FROM england_metrics_pivot\n",
							"    WHERE num_days_ago <= 120\n",
							"    ORDER BY\n",
							"        metric_type,\n",
							"        date\n",
							"    \"\"\"\n",
							").toPandas()\n",
							"\n",
							"g = sns.relplot(\n",
							"    data=df, x=\"date\", y=\"metric_value\", \n",
							"    col=\"metric_type\", style=\"metric_type\", kind=\"line\", palette=\"Set1\", col_wrap=4,\n",
							"    facet_kws={'sharey': False, 'sharex': True}\n",
							")\n",
							"g.fig.suptitle('England 7-day average Cases, Deaths, Vaccinations and Hospital Metrics over the last 120 days', fontsize=16)\n",
							"g.fig.subplots_adjust(top=0.9);\n",
							"\n",
							"# rotate x labels so they're legible\n",
							"tx = g.set_xticklabels(rotation=30)\n",
							"\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 50
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"### Generating a COVID-19 Vaccination Uptake Chart with PyWaffle \n",
							"\n",
							"On the UK Gov's Coronavirus dashboard, the percentage uptake of vaccination (first and second dose) is shown as a *waffle* chart.  We can do the same thing using matplotlib but we need to install the PyWaffle library and then import it ready for use as follows.\n",
							"\n",
							"If the library is not installed then use `pip install PyWaffle` - for a Synapse Spark pool, you can also download the Python package (from ) and upload this as a workload package and then assign this to your Spark pool."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import matplotlib.pyplot as plt\n",
							"from pywaffle import Waffle"
						],
						"outputs": [],
						"execution_count": 51
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"Using the ```c19.nation```  table we need to get the total first and second dose uptake and then divide this by an estimate of the UK's 16+ population to get a percentage.\n",
							"\n",
							"First we take just the latest data using ```MAX(date)``` and then we sum the totals since these are broken down by nation and we need a UK total.  Then we need pivot the results in order to get a Pandas data series that can be used by the PyWaffle graphing component.  Note that sometimes the latest data doesn't contain the cumulative totals so we filter based on the maximum date where these two fields have data.\n",
							"\n",
							"Since the first dose is really shown as a delta on the second dose, then we actually subtract the second dose total from the first for the purposes of this chart, and finally get the percentage that haven't taken the vaccine."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"df = spark.sql(\n",
							"\"\"\"\n",
							"WITH vaccination_totals AS (\n",
							"    SELECT\n",
							"        SUM(cum_people_vaccinated_first_dose_by_publish_date) AS total_first_dose, \n",
							"        SUM(cum_people_vaccinated_second_dose_by_publish_date) AS total_second_dose \n",
							"    FROM c19.nation \n",
							"    WHERE\n",
							"        date = (\n",
							"            SELECT MAX(date) \n",
							"            FROM c19.nation\n",
							"            WHERE\n",
							"                cum_people_vaccinated_first_dose_by_publish_date IS NOT NULL\n",
							"            AND cum_people_vaccinated_second_dose_by_publish_date IS NOT NULL\n",
							"        )\n",
							")\n",
							"SELECT\n",
							"    'Second Dose' AS dose,\n",
							"    total_second_dose/543268.0 AS percentage_uptake\n",
							"FROM vaccination_totals \n",
							"    UNION ALL\n",
							"SELECT\n",
							"    'First Dose' AS dose,\n",
							"    (total_first_dose - total_second_dose)/543268.0 AS percentage_uptake\n",
							"FROM vaccination_totals \n",
							"    \n",
							"    UNION ALL\n",
							"SELECT\n",
							"    'None' AS dose,\n",
							"    (54326800 - total_first_dose)/543268.0 AS percentage_uptake\n",
							"FROM vaccination_totals \n",
							"    \n",
							"--ORDER BY percentage_uptake DESC\n",
							"\"\"\"\n",
							").toPandas()\n",
							"\n",
							"df.head()"
						],
						"outputs": [],
						"execution_count": 52
					},
					{
						"cell_type": "markdown",
						"metadata": {},
						"source": [
							"Now we configure the Waffle chart and pass in the ```percentage_uptake``` series as the data."
						]
					},
					{
						"cell_type": "code",
						"metadata": {},
						"source": [
							"import warnings\n",
							"warnings.filterwarnings(\"ignore\")\n",
							"\n",
							"fig = plt.figure(\n",
							"    FigureClass=Waffle,\n",
							"    rows=10, columns=10, vertical=True, rounding_rule='nearest',\n",
							"    values=df[\"percentage_uptake\"],\n",
							"    title={'label': 'Percentage vaccinated of adult population','loc': 'left','fontdict': {'fontsize': 14}},    \n",
							"    legend={'labels': [\"Second\",\"First\",\"None\"],'loc': 'lower left','bbox_to_anchor': (0, -0.15),'ncol': 3,'framealpha': 0,'fontsize': 12},\n",
							"    colors=[\"#21618C\", \"#3498DB\", \"#D6EAF8\"],\n",
							"    figsize=(10, 5)\n",
							"    )\n",
							"\n",
							"plt.show()"
						],
						"outputs": [],
						"execution_count": 55
					}
				]
			},
			"dependsOn": []
		}
	]
}